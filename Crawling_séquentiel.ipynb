{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTQf6D7pKUkL429UqiHbx1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amine406/SeoProject/blob/main/Crawling_s%C3%A9quentiel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekD-Horth2ry"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import pandas as pd\n",
        "\n",
        "# Initialiser les variables\n",
        "visited_urls = set()  # Pour stocker les URLs visitées\n",
        "internal_urls = set()  # Pour stocker les URLs internes\n",
        "url_info_list = []  # Stocker les infos SEO pour chaque URL\n",
        "\n",
        "# Fonction pour vérifier si l'URL est valide et interne\n",
        "def is_valid_url(url, base_url):\n",
        "    parsed_url = urlparse(url)\n",
        "    return bool(parsed_url.scheme) and bool(parsed_url.netloc) and base_url in url\n",
        "\n",
        "# Fonction pour crawler et analyser une pagea\n",
        "def crawl_page(url, base_url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        status_code = response.status_code\n",
        "        print(f\"Crawling {url} (status code: {status_code})\")\n",
        "\n",
        "        # Vérifier que la réponse est bien du HTML\n",
        "        if \"text/html\" not in response.headers[\"Content-Type\"]:\n",
        "            print(f\"Skipping non-HTML content at {url}\")\n",
        "            return [] # Return an empty list if not HTML content\n",
        "\n",
        "        # Forcer l'encodage si nécessaire\n",
        "        response.encoding = 'utf-8'  # Forcer l'encodage UTF-8\n",
        "\n",
        "        # Utiliser un parser robuste\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")  # Ou \"lxml\"\n",
        "\n",
        "        # Récupérer les informations SEO\n",
        "        title = soup.title.string if soup.title else \"No title\"\n",
        "        description = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
        "        description = description[\"content\"] if description else \"No description\"\n",
        "        h1 = soup.find(\"h1\")\n",
        "        h1_text = h1.get_text() if h1 else \"No H1\"\n",
        "\n",
        "        # Compter le nombre de liens sur la page\n",
        "        links = soup.find_all(\"a\", href=True)\n",
        "        num_links = len(links)\n",
        "\n",
        "        # Stocker les informations dans une liste\n",
        "        url_info_list.append({\n",
        "            \"URL\": url,\n",
        "            \"HTTP Status\": status_code,\n",
        "            \"Title\": title,\n",
        "            \"Meta Description\": description,\n",
        "            \"H1\": h1_text,\n",
        "            \"Number of Links\": num_links\n",
        "        })\n",
        "\n",
        "        # Retourner la liste des liens internes\n",
        "        return [urljoin(base_url, link.get('href')) for link in links if is_valid_url(urljoin(base_url, link.get('href')), base_url)]\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Failed to crawl {url}: {e}\")\n",
        "        return [] # Return an empty list if there is an error\n",
        "\n",
        "# Fonction principale pour démarrer le crawling\n",
        "def crawl_website(start_url):\n",
        "    base_url = \"{0.scheme}://{0.netloc}\".format(urlparse(start_url))\n",
        "    to_crawl = [start_url]\n",
        "\n",
        "    while to_crawl:\n",
        "        url = to_crawl.pop()\n",
        "\n",
        "        if url not in visited_urls:\n",
        "            visited_urls.add(url)\n",
        "            new_links = crawl_page(url, base_url)\n",
        "\n",
        "            # Ajouter les nouveaux liens à la pile des URLs à crawler\n",
        "            to_crawl.extend(new_links)\n",
        "\n",
        "    # Convertir les données collectées en DataFrame\n",
        "    df = pd.DataFrame(url_info_list)\n",
        "\n",
        "    # Afficher le tableau\n",
        "    print(\"\\nTableau récapitulatif des pages crawlées:\")\n",
        "    print(df)\n",
        "\n",
        "    # Exporter vers un fichier CSV si besoin\n",
        "    df.to_csv(\"seo_crawl_results.csv\", index=False)\n",
        "\n",
        "# Démarrer le crawler sur le site \"https://aundetailpres.fr\"\n",
        "if __name__ == \"__main__\":\n",
        "    crawl_website(\"https://aundetailpres.fr\")"
      ]
    }
  ]
}