{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkxJ/4/IzB6WWp99dfG4O5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amine406/SeoProject/blob/main/Crawling_Multi_thread%C3%A9_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4Y8HQuFjEnq"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import pandas as pd\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# Initialiser les variables\n",
        "visited_urls = set()  # Pour stocker les URLs visitées\n",
        "url_info_list = []  # Stocker les infos SEO pour chaque URL\n",
        "\n",
        "# Créer une session globale\n",
        "session = requests.Session()\n",
        "\n",
        "# Ajouter un User-Agent à la session\n",
        "session.headers.update({\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "})\n",
        "\n",
        "# Fonction pour vérifier si l'URL est valide et interne\n",
        "def is_valid_url(url, base_url):\n",
        "    parsed_url = urlparse(url)\n",
        "    return bool(parsed_url.scheme) and bool(parsed_url.netloc) and base_url in url\n",
        "\n",
        "# Fonction pour crawler et analyser une page\n",
        "def crawl_page(url, base_url):\n",
        "    try:\n",
        "        response = session.get(url, timeout=5)  # Timeout après 5 secondes\n",
        "        status_code = response.status_code\n",
        "        print(f\"Crawling {url} {status_code}\")\n",
        "\n",
        "        if status_code != 200:\n",
        "            return []  # Ignorer si le status n'est pas OK\n",
        "\n",
        "        # Vérifier que la réponse est bien du HTML\n",
        "        content_type = response.headers.get(\"Content-Type\", \"\")\n",
        "        print(f\"Content-Type de {url}: {content_type}\")\n",
        "        if \"text/html\" not in content_type:\n",
        "            return []  # Ignorer le contenu non-HTML\n",
        "\n",
        "        # Utiliser un parser robuste\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        # Récupérer les informations SEO\n",
        "        title = soup.title.string if soup.title else \"No title\"\n",
        "        description = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
        "        description = description[\"content\"] if description else \"No description\"\n",
        "        h1 = soup.find(\"h1\")\n",
        "        h1_text = h1.get_text() if h1 else \"No H1\"\n",
        "\n",
        "        # Compter le nombre de liens sur la page\n",
        "        links = soup.find_all(\"a\", href=True)\n",
        "        num_links = len(links)\n",
        "        print(f\"{num_links} liens trouvés sur {url}\")\n",
        "\n",
        "        # Stocker les informations dans une liste\n",
        "        url_info_list.append({\n",
        "            \"URL\": url,\n",
        "            \"HTTP Status\": status_code,\n",
        "            \"Title\": title,\n",
        "            \"Meta Description\": description,\n",
        "            \"H1\": h1_text,\n",
        "            \"Number of Links\": num_links\n",
        "        })\n",
        "\n",
        "        # Retourner la liste des liens internes\n",
        "        return [urljoin(base_url, link.get('href')) for link in links if is_valid_url(urljoin(base_url, link.get('href')), base_url)]\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Failed to crawl {url}: {e}\")\n",
        "        return []  # Return an empty list if there is an error\n",
        "\n",
        "# Fonction principale pour démarrer le crawling\n",
        "def crawl_website(start_url, max_workers=5):\n",
        "    base_url = \"{0.scheme}://{0.netloc}\".format(urlparse(start_url))\n",
        "    to_crawl = [start_url]\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = []\n",
        "\n",
        "        while to_crawl:\n",
        "            url = to_crawl.pop()\n",
        "\n",
        "            if url not in visited_urls:\n",
        "                visited_urls.add(url)\n",
        "                futures.append(executor.submit(crawl_page, url, base_url))\n",
        "\n",
        "        # Attendre que toutes les tâches soient terminées\n",
        "\n",
        "        for future in futures:\n",
        "         new_links = future.result()\n",
        "\n",
        "       # Print each new link on a separate line\n",
        "        print(\"Nouveaux liens trouvés :\")\n",
        "        for link in new_links:\n",
        "               print(link)\n",
        "\n",
        "    # Extend the to_crawl list with the new links\n",
        "    to_crawl.extend(new_links)\n",
        "\n",
        "    # Convertir les données collectées en DataFrame\n",
        "    df = pd.DataFrame(url_info_list)\n",
        "\n",
        "    # Afficher le tableau\n",
        "    print(\"\\nTableau récapitulatif des pages crawlées:\")\n",
        "    print(df)\n",
        "\n",
        "    # Exporter vers un fichier CSV si des données ont été collectées\n",
        "    if not df.empty:\n",
        "        df.to_csv(\"seo_crawl_results_file.csv\", index=False)\n",
        "        print(\"Résultats exportés vers 'seo_crawl_results_file'.\")\n",
        "    else:\n",
        "        print(\"Aucune donnée à exporter.\")\n",
        "\n",
        "# Démarrer le crawler sur le site \"https://aundetailpres.fr\"\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Démarrage du crawling sur https://aundetailpres.fr...\")\n",
        "    crawl_website(\"https://aundetailpres.fr\", max_workers=10)\n",
        "    print(\"Crawling terminé.\")"
      ]
    }
  ]
}