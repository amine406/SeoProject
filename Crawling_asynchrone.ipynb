{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWBzQvvwThz3kt63q8CzOR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amine406/SeoProject/blob/main/Crawling_asynchrone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBoD7uLGkYza"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import pandas as pd\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# Initialiser les variables\n",
        "visited_urls = set()  # Pour stocker les URLs visitées\n",
        "url_info_list = []     # Stocker les infos SEO pour chaque URL\n",
        "\n",
        "# Créer une session globale\n",
        "session = requests.Session()\n",
        "\n",
        "# Ajouter un User-Agent à la session\n",
        "session.headers.update({\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "})\n",
        "\n",
        "# Fonction pour vérifier si l'URL est valide et interne\n",
        "def is_valid_url(url, base_url):\n",
        "    parsed_url = urlparse(url)\n",
        "    return bool(parsed_url.scheme) and bool(parsed_url.netloc) and base_url in url\n",
        "\n",
        "# Fonction pour crawler et analyser une page\n",
        "def crawl_page(url, base_url):\n",
        "    try:\n",
        "        response = session.get(url, timeout=10)  # Timeout après 10 secondes\n",
        "        status_code = response.status_code\n",
        "        print(f\"Crawling {url} {status_code}\")\n",
        "\n",
        "        if status_code != 200:\n",
        "            return []  # Ignorer si le statut n'est pas OK\n",
        "\n",
        "        # Vérifier que la réponse est bien du HTML\n",
        "        content_type = response.headers.get(\"Content-Type\", \"\")\n",
        "        if \"text/html\" not in content_type:\n",
        "            return []  # Ignorer le contenu non-HTML\n",
        "\n",
        "        # Utiliser un parser robuste\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        # Récupérer les informations SEO\n",
        "        title = soup.title.string.strip() if soup.title and soup.title.string else \"No title\"\n",
        "        description_tag = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
        "        description = description_tag[\"content\"].strip() if description_tag and description_tag.get(\"content\") else \"No description\"\n",
        "        h1_tag = soup.find(\"h1\")\n",
        "        h1_text = h1_tag.get_text(strip=True) if h1_tag else \"No H1\"\n",
        "\n",
        "        # Compter le nombre de liens sur la page\n",
        "        links = soup.find_all(\"a\", href=True)\n",
        "        num_links = len(links)\n",
        "\n",
        "        # Trouver les nouveaux liens internes\n",
        "        new_links = [\n",
        "            urljoin(base_url, link.get('href'))\n",
        "            for link in links\n",
        "            if is_valid_url(urljoin(base_url, link.get('href')), base_url)\n",
        "        ]\n",
        "\n",
        "        # Message des nouveaux liens trouvés\n",
        "        message_nouveaux_liens = f\"Nouveaux liens trouvés :\\n\" + \"\\n\".join(new_links)\n",
        "\n",
        "        # Ajouter les informations SEO et les nouveaux liens au tableau\n",
        "        url_info_list.append({\n",
        "            \"URL\": url,\n",
        "            \"HTTP Status\": status_code,\n",
        "            \"Title\": title,\n",
        "            \"Meta Description\": description,\n",
        "            \"H1\": h1_text,\n",
        "            \"Number of Links\": num_links,\n",
        "            \"New Links Found\": \"\\n\".join(new_links),  # Retour à la ligne pour chaque nouveau lien\n",
        "            \"Message Nouveaux Liens\": message_nouveaux_liens\n",
        "        })\n",
        "\n",
        "        # Retourner les nouveaux liens pour le crawling\n",
        "        return new_links\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Failed to crawl {url}: {e}\")\n",
        "        return []  # Retourner une liste vide en cas d'erreur\n",
        "\n",
        "# Fonction principale pour démarrer le crawling sans limitation de profondeur avec parallélisation\n",
        "def crawl_website(start_url, max_workers=10):\n",
        "    base_url = \"{0.scheme}://{0.netloc}\".format(urlparse(start_url))\n",
        "    to_crawl = [start_url]  # Liste des URLs à crawler\n",
        "    visited_urls.add(start_url)\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        # Dictionnaire pour suivre les futures\n",
        "        future_to_url = {executor.submit(crawl_page, url, base_url): url for url in to_crawl}\n",
        "\n",
        "        while future_to_url:\n",
        "            # Attendre que les futures soient terminées\n",
        "            done, _ = as_completed(future_to_url), None\n",
        "            for future in done:\n",
        "                url = future_to_url[future]\n",
        "                try:\n",
        "                    new_links = future.result()\n",
        "                    print(f\"Nouveaux liens trouvés : {new_links}\")\n",
        "\n",
        "                    # Ajouter les nouveaux liens à la liste des URLs à crawler\n",
        "                    for link in new_links:\n",
        "                        if link not in visited_urls:\n",
        "                            visited_urls.add(link)\n",
        "                            future_to_url[executor.submit(crawl_page, link, base_url)] = link\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {url}: {e}\")\n",
        "\n",
        "                # Retirer le future terminé du dictionnaire\n",
        "                del future_to_url[future]\n",
        "                break  # Recommencer la boucle while\n",
        "\n",
        "    # Convertir les données collectées en DataFrame\n",
        "    df = pd.DataFrame(url_info_list)\n",
        "\n",
        "    # Afficher le tableau\n",
        "    print(\"\\nTableau récapitulatif des pages crawlées:\")\n",
        "    print(df)\n",
        "\n",
        "    # Exporter vers un fichier CSV si des données ont été collectées\n",
        "    if not df.empty:\n",
        "        df.to_csv(\"seo_crawl_results_file3.csv\", index=False)\n",
        "        print(\"Résultats exportés vers 'seo_crawl_results_file3.csv'.\")\n",
        "    else:\n",
        "        print(\"Aucune donnée à exporter.\")\n",
        "\n",
        "    # Démarrer le crawler sur le site \"https://aundetailpres.fr\"\n",
        "if __name__ == \"__main__\": # removed extra indent\n",
        "    print(\"Démarrage du crawling sur https://aundetailpres.fr...\")\n",
        "    crawl_website(\"https://aundetailpres.fr\")\n",
        "    print(\"Crawling terminé.\")"
      ]
    }
  ]
}